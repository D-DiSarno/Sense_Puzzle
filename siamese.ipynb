{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:27.716377Z",
     "start_time": "2017-12-28T00:14:27.193111Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable   \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:30.362283Z",
     "start_time": "2017-12-28T00:14:27.718293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "CUDA_FLAG = torch.cuda.is_available()\n",
    "print(CUDA_FLAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:30.389778Z",
     "start_time": "2017-12-28T00:14:30.364617Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA_FLAG:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "SEQ_LEN = 465\n",
    "EMBEDDING_LEN = 120\n",
    "NUM_TRAINING_PAIRS = 20 * 500\n",
    "NUM_EPOCH = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "class Config():\n",
    "    train_data_fp = './converted/pair_shuffle.fa'\n",
    "    train_target_fp = './converted/dist_shuffle.txt'\n",
    "    train_num_example = NUM_TRAINING_PAIRS\n",
    "    train_batch_size = BATCH_SIZE\n",
    "    num_epoch = NUM_EPOCH\n",
    "    learning_rate =LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:30.471322Z",
     "start_time": "2017-12-28T00:14:30.391872Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaxMinout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, embedding1, embedding2):\n",
    "        shape = list(embedding1.size())\n",
    "        flat1 = embedding1.view(1, -1)\n",
    "        flat2 = embedding2.view(1, -1)\n",
    "        combined = torch.cat((flat1, flat2), 0)\n",
    "        maxout = combined.max(0)[0].view(*shape)\n",
    "        # minout = combined.min(0)[0].view(*shape)\n",
    "        minout = ((combined * -1).max(0)[0].view(*shape) * -1) # workaround for memory leak bug\n",
    "\n",
    "        return maxout, minout\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxminout = MaxMinout()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4, 16, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "                \n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "               \n",
    "            nn.Conv1d(32, 48, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        max_pooling_len = SEQ_LEN \n",
    "        max_pooling_len = np.floor((max_pooling_len - 2) / 2 + 1)\n",
    "        max_pooling_len = np.floor((max_pooling_len - 2) / 2 + 1)\n",
    "        max_pooling_len = np.floor((max_pooling_len - 2) / 2 + 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(int(48 * max_pooling_len), EMBEDDING_LEN),\n",
    "        )\n",
    "\n",
    "    def forward_one_side(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        embedding1 = self.forward_one_side(input1)\n",
    "        embedding2 = self.forward_one_side(input2)\n",
    "        maxout, minout = self.maxminout(embedding1, embedding2)\n",
    "        return maxout, minout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:30.485665Z",
     "start_time": "2017-12-28T00:14:30.473034Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_func(dist):\n",
    "    return 1.0\n",
    "#     return 100.0 if dist < 0.2 else 1.0\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, maxout, minout, align_dist):\n",
    "        weight = Variable(torch.FloatTensor([weight_func(x) for x in align_dist.data]), requires_grad=False)\n",
    "        if CUDA_FLAG:\n",
    "            weight = weight.cuda()\n",
    "        loss_contrastive = torch.mean(torch.mul(weight, torch.pow(1 - minout.sum(1)/maxout.sum(1) - align_dist, 2)))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:14:30.564345Z",
     "start_time": "2017-12-28T00:14:30.487411Z"
    }
   },
   "outputs": [],
   "source": [
    "atcg_map = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, data_fp, target_fp, N):\n",
    "        self.data_fp = data_fp\n",
    "        self.target_fp = target_fp\n",
    "        self.N = N\n",
    "        self.data_tensor = self.gen_data_tensor()\n",
    "        self.target_tensor = self.gen_target_tensor()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[0][index], self.data_tensor[1][index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def gen_data_tensor(self):\n",
    "        seq1 = torch.zeros((self.N, 4, SEQ_LEN))\n",
    "        seq2 = torch.zeros((self.N, 4, SEQ_LEN))\n",
    "        cnt = 0\n",
    "        with open(self.data_fp) as f:\n",
    "            while True:\n",
    "                next_n = list(islice(f, 4))\n",
    "                if not next_n:\n",
    "                    break\n",
    "                if cnt >= self.N:\n",
    "                    break\n",
    "                read1 = next_n[1].strip()\n",
    "                read2 = next_n[3].strip()\n",
    "                for i, c in enumerate(read1):\n",
    "                    seq1[cnt, atcg_map.get(c, 0), i] = 1.0\n",
    "                for i, c in enumerate(read2):\n",
    "                    seq2[cnt, atcg_map.get(c, 0), i] = 1.0\n",
    "                cnt += 1\n",
    "        return seq1, seq2\n",
    "\n",
    "    def gen_target_tensor(self):\n",
    "        target = torch.zeros(self.N)\n",
    "        with open(self.target_fp) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= self.N:\n",
    "                    break\n",
    "                pair_id, dist = line.strip().split()\n",
    "                target[i] = float(dist)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:16:46.806114Z",
     "start_time": "2017-12-28T00:14:30.566261Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (maxminout): MaxMinout()\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(4, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv1d(32, 48, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2784, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SiameseNetwork()    \n",
    "if CUDA_FLAG:\n",
    "    net.cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=Config.learning_rate, weight_decay=0)\n",
    "print(net)\n",
    "\n",
    "training_dataset = SiameseNetworkDataset(Config.train_data_fp, \n",
    "                                         Config.train_target_fp,\n",
    "                                         Config.train_num_example)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset=training_dataset,      \n",
    "    batch_size=Config.train_batch_size,      \n",
    "    shuffle=True,              \n",
    "    num_workers=0,#4              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:33:54.826874Z",
     "start_time": "2017-12-28T00:16:47.998046Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========>\n",
      "Epoch: 1/10, Batch: 10/100\n",
      "Accumulated loss: 3.7024e-01\n",
      "Epoch: 1/10, Batch: 20/100\n",
      "Accumulated loss: 3.6586e-01\n",
      "Epoch: 1/10, Batch: 30/100\n",
      "Accumulated loss: 3.5018e-01\n",
      "Epoch: 1/10, Batch: 40/100\n",
      "Accumulated loss: 3.1972e-01\n",
      "Epoch: 1/10, Batch: 50/100\n",
      "Accumulated loss: 2.7521e-01\n",
      "Epoch: 1/10, Batch: 60/100\n",
      "Accumulated loss: 2.3329e-01\n",
      "Epoch: 1/10, Batch: 70/100\n",
      "Accumulated loss: 2.0170e-01\n",
      "Epoch: 1/10, Batch: 80/100\n",
      "Accumulated loss: 1.7774e-01\n",
      "Epoch: 1/10, Batch: 90/100\n",
      "Accumulated loss: 1.5860e-01\n",
      "Epoch: 1/10, Batch: 100/100\n",
      "Accumulated loss: 1.4322e-01\n",
      "Train loss: 1.4322e-01\n",
      "===========>\n",
      "Epoch: 2/10, Batch: 10/100\n",
      "Accumulated loss: 3.7643e-03\n",
      "Epoch: 2/10, Batch: 20/100\n",
      "Accumulated loss: 3.8261e-03\n",
      "Epoch: 2/10, Batch: 30/100\n",
      "Accumulated loss: 3.8060e-03\n",
      "Epoch: 2/10, Batch: 40/100\n",
      "Accumulated loss: 3.5544e-03\n",
      "Epoch: 2/10, Batch: 50/100\n",
      "Accumulated loss: 3.3199e-03\n",
      "Epoch: 2/10, Batch: 60/100\n",
      "Accumulated loss: 3.1775e-03\n",
      "Epoch: 2/10, Batch: 70/100\n",
      "Accumulated loss: 2.9722e-03\n",
      "Epoch: 2/10, Batch: 80/100\n",
      "Accumulated loss: 2.8001e-03\n",
      "Epoch: 2/10, Batch: 90/100\n",
      "Accumulated loss: 2.6920e-03\n",
      "Epoch: 2/10, Batch: 100/100\n",
      "Accumulated loss: 2.5619e-03\n",
      "Train loss: 2.5619e-03\n",
      "===========>\n",
      "Epoch: 3/10, Batch: 10/100\n",
      "Accumulated loss: 1.8542e-03\n",
      "Epoch: 3/10, Batch: 20/100\n",
      "Accumulated loss: 1.6806e-03\n",
      "Epoch: 3/10, Batch: 30/100\n",
      "Accumulated loss: 1.4359e-03\n",
      "Epoch: 3/10, Batch: 40/100\n",
      "Accumulated loss: 1.3808e-03\n",
      "Epoch: 3/10, Batch: 50/100\n",
      "Accumulated loss: 1.4252e-03\n",
      "Epoch: 3/10, Batch: 60/100\n",
      "Accumulated loss: 1.3509e-03\n",
      "Epoch: 3/10, Batch: 70/100\n",
      "Accumulated loss: 1.2976e-03\n",
      "Epoch: 3/10, Batch: 80/100\n",
      "Accumulated loss: 1.2635e-03\n",
      "Epoch: 3/10, Batch: 90/100\n",
      "Accumulated loss: 1.2305e-03\n",
      "Epoch: 3/10, Batch: 100/100\n",
      "Accumulated loss: 1.2425e-03\n",
      "Train loss: 1.2425e-03\n",
      "===========>\n",
      "Epoch: 4/10, Batch: 10/100\n",
      "Accumulated loss: 8.9130e-04\n",
      "Epoch: 4/10, Batch: 20/100\n",
      "Accumulated loss: 9.6593e-04\n",
      "Epoch: 4/10, Batch: 30/100\n",
      "Accumulated loss: 9.2972e-04\n",
      "Epoch: 4/10, Batch: 40/100\n",
      "Accumulated loss: 1.0179e-03\n",
      "Epoch: 4/10, Batch: 50/100\n",
      "Accumulated loss: 1.0299e-03\n",
      "Epoch: 4/10, Batch: 60/100\n",
      "Accumulated loss: 9.9979e-04\n",
      "Epoch: 4/10, Batch: 70/100\n",
      "Accumulated loss: 9.9741e-04\n",
      "Epoch: 4/10, Batch: 80/100\n",
      "Accumulated loss: 9.9242e-04\n",
      "Epoch: 4/10, Batch: 90/100\n",
      "Accumulated loss: 9.5802e-04\n",
      "Epoch: 4/10, Batch: 100/100\n",
      "Accumulated loss: 9.5655e-04\n",
      "Train loss: 9.5655e-04\n",
      "===========>\n",
      "Epoch: 5/10, Batch: 10/100\n",
      "Accumulated loss: 7.2894e-04\n",
      "Epoch: 5/10, Batch: 20/100\n",
      "Accumulated loss: 7.1745e-04\n",
      "Epoch: 5/10, Batch: 30/100\n",
      "Accumulated loss: 7.2978e-04\n",
      "Epoch: 5/10, Batch: 40/100\n",
      "Accumulated loss: 7.8426e-04\n",
      "Epoch: 5/10, Batch: 50/100\n",
      "Accumulated loss: 7.8169e-04\n",
      "Epoch: 5/10, Batch: 60/100\n",
      "Accumulated loss: 7.7222e-04\n",
      "Epoch: 5/10, Batch: 70/100\n",
      "Accumulated loss: 7.7967e-04\n",
      "Epoch: 5/10, Batch: 80/100\n",
      "Accumulated loss: 7.8300e-04\n",
      "Epoch: 5/10, Batch: 90/100\n",
      "Accumulated loss: 7.9089e-04\n",
      "Epoch: 5/10, Batch: 100/100\n",
      "Accumulated loss: 7.8683e-04\n",
      "Train loss: 7.8683e-04\n",
      "===========>\n",
      "Epoch: 6/10, Batch: 10/100\n",
      "Accumulated loss: 6.1826e-04\n",
      "Epoch: 6/10, Batch: 20/100\n",
      "Accumulated loss: 6.3810e-04\n",
      "Epoch: 6/10, Batch: 30/100\n",
      "Accumulated loss: 6.5546e-04\n",
      "Epoch: 6/10, Batch: 40/100\n",
      "Accumulated loss: 6.5912e-04\n",
      "Epoch: 6/10, Batch: 50/100\n",
      "Accumulated loss: 6.7414e-04\n",
      "Epoch: 6/10, Batch: 60/100\n",
      "Accumulated loss: 6.7334e-04\n",
      "Epoch: 6/10, Batch: 70/100\n",
      "Accumulated loss: 6.6515e-04\n",
      "Epoch: 6/10, Batch: 80/100\n",
      "Accumulated loss: 6.6466e-04\n",
      "Epoch: 6/10, Batch: 90/100\n",
      "Accumulated loss: 6.7406e-04\n",
      "Epoch: 6/10, Batch: 100/100\n",
      "Accumulated loss: 6.7176e-04\n",
      "Train loss: 6.7176e-04\n",
      "===========>\n",
      "Epoch: 7/10, Batch: 10/100\n",
      "Accumulated loss: 6.8327e-04\n",
      "Epoch: 7/10, Batch: 20/100\n",
      "Accumulated loss: 6.0954e-04\n",
      "Epoch: 7/10, Batch: 30/100\n",
      "Accumulated loss: 6.1361e-04\n",
      "Epoch: 7/10, Batch: 40/100\n",
      "Accumulated loss: 5.9777e-04\n",
      "Epoch: 7/10, Batch: 50/100\n",
      "Accumulated loss: 6.0549e-04\n",
      "Epoch: 7/10, Batch: 60/100\n",
      "Accumulated loss: 5.9605e-04\n",
      "Epoch: 7/10, Batch: 70/100\n",
      "Accumulated loss: 6.0861e-04\n",
      "Epoch: 7/10, Batch: 80/100\n",
      "Accumulated loss: 6.0469e-04\n",
      "Epoch: 7/10, Batch: 90/100\n",
      "Accumulated loss: 6.0613e-04\n",
      "Epoch: 7/10, Batch: 100/100\n",
      "Accumulated loss: 5.9677e-04\n",
      "Train loss: 5.9677e-04\n",
      "===========>\n",
      "Epoch: 8/10, Batch: 10/100\n",
      "Accumulated loss: 6.0125e-04\n",
      "Epoch: 8/10, Batch: 20/100\n",
      "Accumulated loss: 5.7427e-04\n",
      "Epoch: 8/10, Batch: 30/100\n",
      "Accumulated loss: 5.4067e-04\n",
      "Epoch: 8/10, Batch: 40/100\n",
      "Accumulated loss: 5.2090e-04\n",
      "Epoch: 8/10, Batch: 50/100\n",
      "Accumulated loss: 5.2453e-04\n",
      "Epoch: 8/10, Batch: 60/100\n",
      "Accumulated loss: 5.3730e-04\n",
      "Epoch: 8/10, Batch: 70/100\n",
      "Accumulated loss: 5.5279e-04\n",
      "Epoch: 8/10, Batch: 80/100\n",
      "Accumulated loss: 5.5145e-04\n"
     ]
    }
   ],
   "source": [
    "train_num_batch = int(np.ceil(Config.train_num_example / Config.train_batch_size))\n",
    "train_batch_interval = Config.train_num_example // Config.train_batch_size // 10\n",
    "train_loss_hist = []\n",
    "\n",
    "for epoch in range(Config.num_epoch):\n",
    "    print('===========>')\n",
    "    train_running_loss = 0\n",
    "    for batch_index, (train_seq1, train_seq2, train_target) in enumerate(training_loader):\n",
    "        if CUDA_FLAG:\n",
    "            train_seq1 = train_seq1.cuda()\n",
    "            train_seq2 = train_seq2.cuda()\n",
    "            train_target = train_target.cuda()\n",
    "        train_seq1 = Variable(train_seq1)\n",
    "        train_seq2 = Variable(train_seq2)\n",
    "        train_target = Variable(train_target).float()\n",
    "        train_output1, train_output2 = net(train_seq1, train_seq2)\n",
    "        train_loss_contrastive = criterion(train_output1, train_output2, train_target)\n",
    "        #train_running_loss += train_loss_contrastive.data[0]\n",
    "        train_running_loss += train_loss_contrastive.item()\n",
    "        \n",
    "        if batch_index % train_batch_interval == train_batch_interval - 1:\n",
    "            print('Epoch: {:d}/{:d}, Batch: {:d}/{:d}\\n'\n",
    "                  'Accumulated loss: {:.4e}'.format(\n",
    "                  epoch + 1, Config.num_epoch, \n",
    "                  batch_index + 1, train_num_batch, \n",
    "                  train_running_loss / (batch_index + 1)))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss_contrastive.backward()\n",
    "        optimizer.step() \n",
    "    train_loss = train_running_loss / train_num_batch\n",
    "    train_loss_hist.append(train_loss)\n",
    "    print('Train loss: {:.4e}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:33:54.992612Z",
     "start_time": "2017-12-28T00:33:54.829720Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_loss_hist[1:], 'b')\n",
    "plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:33:55.071137Z",
     "start_time": "2017-12-28T00:33:54.997359Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_dist(embedding1, embedding2):\n",
    "    return 1 - np.sum(np.minimum(embedding1, embedding2)) / np.sum(np.maximum(embedding1, embedding2)) \n",
    "    \n",
    "def pair_dist(fp, N, embedding_fp, dist_fp):\n",
    "    seq = torch.zeros((N, 4, SEQ_LEN))\n",
    "    cnt = 0\n",
    "    seq_ids = []\n",
    "    with open(fp) as f:\n",
    "        while True:\n",
    "            next_n = list(islice(f, 2))\n",
    "            if not next_n:\n",
    "                break\n",
    "            seq_id = next_n[0].strip()[1:]\n",
    "            read = next_n[1].strip()\n",
    "            seq_ids.append(seq_id)\n",
    "            for i, c in enumerate(read):\n",
    "                seq[cnt, atcg_map.get(c, 0), i] = 1.0\n",
    "            cnt += 1\n",
    "    embeddings = net.forward_one_side(Variable(seq)).data.numpy()\n",
    "    embeddings.tofile(embedding_fp, sep=',', format='%.4e')\n",
    "    with open(dist_fp, 'w') as fo:\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i < j:\n",
    "                    fo.write('{}-{}\\t{:.4f}\\n'.format(\n",
    "                        seq_ids[i], seq_ids[j],\n",
    "                        jaccard_dist(embeddings[i],\n",
    "                                     embeddings[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:33:55.109974Z",
     "start_time": "2017-12-28T00:33:55.072959Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def my_plot(align_dist_df, x_dist_df, save_fp):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    vmin = align_dist_df[1].min()#\n",
    "    vmax = align_dist_df[1].max()#\n",
    "    percentile = 99#\n",
    "    hb = ax.hexbin(align_dist_df[1], x_dist_df[1], \n",
    "                   gridsize=200, bins='log', cmap='Blues', extent=(0, 1, 0, 1),\n",
    "                   vmin=np.percentile(align_dist_df[1], 100 - percentile),\n",
    "                   vmax=np.percentile(align_dist_df[1], percentile))\n",
    "                   #vmin=0, vmax=4)\n",
    "    ax.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), 'r')\n",
    "    ax.set_xlabel('alignment distance', fontsize=20)\n",
    "    ax.set_ylabel('SENSE', fontsize=20)\n",
    "    \n",
    "    cbar_ax = fig.add_axes([0.95, 0.1, 0.05, 0.8])\n",
    "    cbar_ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    cbar = fig.colorbar(hb, cax=cbar_ax)\n",
    "    cbar.set_label('log10(count + 1)', fontsize=20)\n",
    "    fig.savefig(save_fp, bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T00:39:51.475677Z",
     "start_time": "2017-12-28T00:33:55.111735Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "NUM_EVAL_N = 500 # number of eval pairs is N*(N-1)/2\n",
    "pair_dist('./converted/eval.fa', NUM_EVAL_N, './converted/embeddings.txt', './converted/embeddings_dist.txt')\n",
    "nw_df = pd.read_csv('./converted/eval_dist.txt', sep='\\t', header=None)\n",
    "my_df = pd.read_csv('./converted/embeddings_dist.txt', sep='\\t', header=None)\n",
    "my_plot(nw_df, my_df, save_fp='./converted/demo.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
